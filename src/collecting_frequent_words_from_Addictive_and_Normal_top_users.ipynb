{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import time, re\n",
    "import sqlite3\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "con = sqlite3.connect('C:/Users/Pierre Claisse/Documents/NTHU/Data Mining in Social Networks/Final project/reddit-comments-may-2015/database.sqlite')\n",
    "cachedStopWords = stopwords.words('english')\n",
    "unnecessary = ['none', 'many', 'much', 'com', 'https', 'one', 'http', 'would', 'get', 'know', 'time', 'still', 'think', 'people', 'got', 'www', 'right', 'yeah', 'lol', 'said', 'yes', 'never', 'jpg']\n",
    "\n",
    "#*******************************************************************************************************************************\n",
    "\n",
    "top_users = ['Lenticious', 'Soulaez', 'TweetsInCommentsBot', 'armiechedon', 'Protopulse', 'sufficiency_bot']\n",
    "All_authors_comments = []\n",
    "\n",
    "\"\"\"\n",
    "sentence_1 = ''\n",
    "words = {}\n",
    "for i in top_users:\n",
    "    with con:  \n",
    "            cur = con.cursor() \n",
    "            s = \"SELECT author,body FROM May2015 WHERE author = '%s' \" %i \n",
    "            cur.execute(s)\n",
    "            rows = cur.fetchall()\n",
    "            for row in rows:\n",
    "                    row = list(row) # as every row is a tuple so converting tuple to list\n",
    "                    row[1] = re.sub(\"[^a-zA-Z]\", \" \", row[1])\n",
    "                    row[1] = ' '.join([word for word in row[1].lower().split() if word not in \\\n",
    "                         cachedStopWords and len(word) > 2 and word not in unnecessary])\n",
    "                    row[1] = row[1].split()               \n",
    "                    for each in row[1]: # for counting the words\n",
    "                            if each not in words:\n",
    "                                words[each] = 1\n",
    "                                sentence_1 += each + \" \"\n",
    "                            else:\n",
    "                                words[each] += 1\n",
    "                                sentence_1 += each + \" \"\n",
    "    \n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "addictive = words[0:40000]\n",
    "print(addictive)\n",
    "All_authors_comments.append(addictive)\n",
    "\"\"\"\n",
    "\n",
    "#***********************************************************************************************\n",
    "\n",
    "sentence_2 = ''\n",
    "words_1 = {}\n",
    "with con:  \n",
    "            cur = con.cursor() \n",
    "            s = \"SELECT author, body FROM May2015 WHERE author NOT IN('AutoModerator', 'Lenticious', 'Soulaez', 'TweetsInCommentsBot', 'armiechedon', 'Protopulse', 'sufficiency_bot') LIMIT 80000\"\n",
    "            cur.execute(s)\n",
    "            rows = cur.fetchall()\n",
    "            for row in rows:\n",
    "                    row = list(row) # as every row is a tuple so converting tuple to list\n",
    "                    row[1] = re.sub(\"[^a-zA-Z]\", \" \", row[1])\n",
    "                    row[1] = ' '.join([word for word in row[1].lower().split() if word not in cachedStopWords and len(word) > 2 and word not in unnecessary])\n",
    "                    row[1] = row[1].split()\n",
    "                    for each in row[1]: # for counting the words\n",
    "                            if (each not in words_1):\n",
    "                                if (each not in words):\n",
    "                                    words_1[each] = 1\n",
    "                                    sentence_2 += each + \" \"\n",
    "                                else :\n",
    "                                    continue\n",
    "                            else:\n",
    "                                words_1[each] += 1\n",
    "                                sentence_2 += each + \" \"\n",
    "    \n",
    "words_1 = sorted(words_1, key=words_1.get, reverse=True)\n",
    "normal = words_1[0:2500]\n",
    "print(normal)\n",
    "All_authors_comments.append(normal)\n",
    "\n",
    "#************************************************************************************************************\n",
    "\n",
    "\"\"\"\n",
    "fh = open('frequent200_words_of_two_group_authors.txt', 'w')\n",
    "for i, each in enumerate(All_authors_comments):\n",
    "    fh.write(\"author\" + str(i) + str(each) + \"\\n\\n\")\n",
    "fh.close()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "fs1 = open('frequent40000_words_addictive.txt', 'w')\n",
    "for i, each in enumerate(addictive):\n",
    "    fs1.write(str(each) + \" \")\n",
    "fs1.close()\n",
    "\"\"\"\n",
    "\n",
    "fs2 = open('2500_words_normal.txt', 'w')\n",
    "for i, each in enumerate(normal):\n",
    "    fs2.write(str(each) + \" \")\n",
    "fs2.close()\n",
    "\n",
    "elapsed = time.time() - startTime\n",
    "print('It spends ' + str( elapsed) + ' seconds\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
